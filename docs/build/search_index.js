var documenterSearchIndex = {"docs":
[{"location":"index.html#","page":"HOME","title":"HOME","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"index.html#TSML-(Time-Series-Machine-Learning)-1","page":"HOME","title":"TSML (Time-Series Machine Learning)","text":"","category":"section"},{"location":"index.html#","page":"HOME","title":"HOME","text":"TSML (Time Series Machine Learning) is package  for Time Series data processing, classification, and prediction. It combines ML libraries from Python's  ScikitLearn, R's Caret, and Julia ML using a common API  and allows seamless ensembling and integration of  heterogenous ML libraries to create complex models  for robust time-series pre-processing and prediction/classification.","category":"page"},{"location":"index.html#Package-Features-1","page":"HOME","title":"Package Features","text":"","category":"section"},{"location":"index.html#","page":"HOME","title":"HOME","text":"TS aggregation based on time/date interval\nTS imputation based on Nearest Neighbors\nTS statistical metrics of data quality\nTS classification for automatic data discovery\nTS prediction with more than 100+ libraries from caret, scikitlearn, and julia\nTS date/val matrix conversion of 1-d TS using sliding windows for ML input\nPipeline API allows high-level description of the processing workflow\nEasily extensible architecture by using just two main interfaces: fit and transform\nSupport for hundreds of external ML libs from Scikitlearn and Caret by using common API wrappers for PyCall and RCall","category":"page"},{"location":"index.html#Installation-1","page":"HOME","title":"Installation","text":"","category":"section"},{"location":"index.html#","page":"HOME","title":"HOME","text":"TSML is in the Julia Official package registry.  The latest release can be installed at the Julia  prompt using Julia's package management which is triggered by pressing ] at the julia prompt:","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"julia> ]\n(v1.0) pkg> add TSML","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> pkg\"add TSML\"","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> Pkg.add(\"TSML\")","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"julia> pkg\"add TSML\"","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"Once TSML is installed, you can load the TSML package by:","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"julia> using TSML","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"julia> import TSML","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"Generally, you will need the different transformers and utils in TSML for time-series processing. To use them, it is standard in TSML code to have the following declared at the topmost part of your application:","category":"page"},{"location":"index.html#","page":"HOME","title":"HOME","text":"using TSML \nusing TSML.TSMLTransformers\nusing TSML.TSMLTypes\nusing TSML.Utils","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/aggregators.html#Aggregators-and-Imputers-1","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"","category":"section"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"The package assumes a two-column table composed of Dates and Values.  The first part of the workflow aggregates values based on the specified  date-time interval which minimizes occurence of missing values and noise.  The aggregated data is then left-joined to the complete sequence of  DateTime  in a specified date-time interval. Remaining missing values are replaced  by k nearest neighbors where k is the symmetric distance from the location  of missing value. This replacement algo is called several times until there  are no more missing values.","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Let us create a Date, Value table with some missing values and output the first 15 rows. We will then apply some TSML functions to normalize/clean the data:","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using Random, Dates, DataFrames\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[:Value][gndxmissing] .= missing\n   return df\nend\n\nX = generateDataWithMissing()\nfirst(X,15)","category":"page"},{"location":"tutorial/aggregators.html#DateValgator-1","page":"Aggregators and Imputers","title":"DateValgator","text":"","category":"section"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"You'll notice several blocks of missing in the table above with reading frequency of every 15 minutes.  To minimize noise and lessen the occurrence of missing values, let's aggregate our dataset by taking the hourly median using the DateValgator transformer.","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML\nusing TSML.TSMLTypes\nusing TSML.Utils\nusing TSML.TSMLTransformers\nusing TSML: DateValgator\n\ndtvlgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nfit!(dtvlgator,X)\nresults = transform!(dtvlgator,X)\nfirst(results,10)","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"The occurrence of missing values is now reduced because of the hourly aggregation. While the default is hourly aggregation, you can easily change it by using a different interval in the argument during instance creation. Below indicates every 30 minutes interval.","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"dtvlgator = DateValgator(Dict(:dateinterval=>Dates.Minute(30)))","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"DateValgator is one of the several TSML transformers to preprocess and clean the  time series data. In order to create additional transformers to extend TSML,  each transformer must overload the two Transformer functions:fit! and transform!.  DateValgator fit! performs initial setups of necessary parameters and validation of arguments while its transform! function contains the algorithm  for aggregation. ","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"For machine learning prediction and classification transformer,  fit! function is equivalent to ML training or parameter optimization,  while the transform! function is for doing the actual prediction. The later part of the tutorial will provide an example how to add a Transformer to extend the functionality of TSML.","category":"page"},{"location":"tutorial/aggregators.html#DateValNNer-1","page":"Aggregators and Imputers","title":"DateValNNer","text":"","category":"section"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"Let's perform further processing to replace the remaining missing values with their nearest neighbors.  We will use DateValNNer which is a TSML transformer to process the output of DateValgator. DateValNNer can also process non-aggregated data by first running similar workflow of DateValgator before performing its imputation routine.","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML: DateValNNer\n\ndatevalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nfit!(datevalnner, X)\nresults = transform!(datevalnner,X)\nfirst(results,10)","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"After running the DateValNNer, it's guaranteed that there will be no more missing data unless the input are all missing data.","category":"page"},{"location":"tutorial/aggregators.html#DateValizer-1","page":"Aggregators and Imputers","title":"DateValizer","text":"","category":"section"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"One more imputer to replace missing data is DateValizer. It computes the hourly median over 24 hours and use the hour => median hashmap learned to replace missing data using hour as the key. In this implementation, fit! function is doing the training of parameters by computing the medians and save it for the transform! function to use for imputation. It is possible that the hashmap can contain missing values in cases where the pooled hourly median in a particular hour have all missing data. Below is a sample workflow to replace missing data in X with the hourly medians.","category":"page"},{"location":"tutorial/aggregators.html#","page":"Aggregators and Imputers","title":"Aggregators and Imputers","text":"using TSML: DateValizer\n\ndatevalizer = DateValizer(Dict(:dateinterval=>Dates.Hour(1)))\nfit!(datevalizer, X)\nresults = transform!(datevalizer,X)\nfirst(results,10)","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/pipeline.html#Pipeline-1","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"Instead of calling fit! and transform! for each transformer to process time series data, we can use the Pipeline transformer which does this automatically by iterating through the transformers and calling fit! and transform! repeatedly for each transformer in its argument.","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"Let's start again by having a function to generate a time series dataframe with some missing data.","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"using Random, Dates, DataFrames\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[:Value][gndxmissing] .= missing\n   return df\nend\n\nX = generateDataWithMissing()\nfirst(X,15)","category":"page"},{"location":"tutorial/pipeline.html#Workflow-of-Pipeline-1","page":"Pipeline","title":"Workflow of Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"Let's use the pipeline transformer to aggregate and impute:","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"using Dates\nusing TSML\nusing TSML.TSMLTypes\nusing TSML.TSMLTransformers\nusing TSML: Pipeline\nusing TSML: DateValgator\nusing TSML: DateValNNer\n\ndtvalgator = DateValgator(Dict(:dateinterval => Dates.Hour(1)))\ndtvalnner = DateValNNer(Dict(:dateinterval => Dates.Hour(1)))\n\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            dtvalnner\n         ]\n  )\n)\n\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)\nfirst(results,10)","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"Using the Pipeline transformer, it becomes straightforward to process the time series data. It also becomes trivial to extend TSML functionality by adding more transformers and making sure each support the fit! and transform! interfaces. Any new transformer can then be easily added to the Pipeline workflow  without invasively changing the existing codes.","category":"page"},{"location":"tutorial/pipeline.html#Extending-TSML-1","page":"Pipeline","title":"Extending TSML","text":"","category":"section"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"To illustrate how simple it is to add a new transformer, below extends TSML by adding CSVReader transformer and added in the pipeline to process CSV data:","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"using TSML.TSMLTypes\nimport TSML.TSMLTypes.fit!\nimport TSML.TSMLTypes.transform!\n\nusing CSV\n\nmutable struct CSVReader <: Transformer\n    model\n    args\n    function CSVReader(args=Dict())\n        default_args = Dict(\n            :filename => \"\",\n            :dateformat => \"\"\n        )\n        new(nothing,mergedict(default_args,args))\n    end\nend\n\nfunction fit!(csvrdr::CSVReader,x::T=[],y::Vector=[]) where {T<:Union{DataFrame,Vector,Matrix}}\n    fname = csvrdr.args[:filename]\n    fmt = csvrdr.args[:dateformat]\n    (fname != \"\" && fmt != \"\") || error(\"missing filename or date format\")\n    model = csvrdr.args\nend\n\nfunction transform!(csvrdr::CSVReader,x::T=[]) where {T<:Union{DataFrame,Vector,Matrix}}\n    fname = csvrdr.args[:filename]\n    fmt = csvrdr.args[:dateformat]\n    df = CSV.read(fname)\n    ncol(df) == 2 || error(\"dataframe should have only two columns: Date,Value\")\n    rename!(df,names(df)[1]=>:Date,names(df)[2]=>:Value)\n    df[:Date] = DateTime.(df[:Date],fmt)\n    df\nend","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"Instead of passing table X that contains the time series, we will add  an instance of theCSVReader at the start of the array of transformers in the pipeline  to read the csv data. CSVReader transform! function converts the csv time series table into a dataframe, which will be consumed by the next transformer in the pipeline  for processing.","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"fname = joinpath(dirname(pathof(TSML)),\"../data/testdata.csv\")\ncsvreader = CSVReader(Dict(:filename=>fname,:dateformat=>\"d/m/y H:M\"))\nfit!(csvreader)\ncsvdata = transform!(csvreader)\nfirst(csvdata,10)","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"Let us now include the newly created CSVReader in the pipeline to read the csv data and process it by aggregation and imputation.","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"mypipeline = Pipeline(\n  Dict( :transformers => [\n            csvreader,\n            dtvalgator,\n            dtvalnner\n         ]\n  )\n)\n\nfit!(mypipeline)\nresults = transform!(mypipeline)\nfirst(results,10)","category":"page"},{"location":"tutorial/pipeline.html#","page":"Pipeline","title":"Pipeline","text":"Notice that there is no more the need to pass X in the arguments of fit! and transform because the data is now transmitted by the CSVReader instance to the other transformers in the pipeline.","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/statistics.html#Statistical-Metrics-1","page":"Statistical Metrics","title":"Statistical Metrics","text":"","category":"section"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let us again start generating an artificial data with missing values which we  will use in our tutorial below.","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"using Random, Dates, DataFrames\nfunction generateDataWithMissing()\n   Random.seed!(123)\n   gdate = DateTime(2014,1,1):Dates.Minute(15):DateTime(2016,1,1)\n   gval = Array{Union{Missing,Float64}}(rand(length(gdate)))\n   gmissing = 50000\n   gndxmissing = Random.shuffle(1:length(gdate))[1:gmissing]\n   df = DataFrame(Date=gdate,Value=gval)\n   df[:Value][gndxmissing] .= missing\n   return df\nend\n\nX = generateDataWithMissing()\nfirst(X,15)","category":"page"},{"location":"tutorial/statistics.html#Statifier-for-Both-Non-Missing-and-Missing-Values-1","page":"Statistical Metrics","title":"Statifier for Both Non-Missing and Missing Values","text":"","category":"section"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"TSML includes Statifier transformer that computes scalar statistics to characterize the time series data. By default, it also computes statistics of  missing blocks of data. To disable this feature, one can pass  :processmissing => false to the argument during its instance creation. Below illustrates this workflow.","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"using Dates\nusing TSML\nusing TSML.TSMLTypes\nusing TSML.TSMLTransformers\nusing TSML: Pipeline\nusing TSML: DateValgator\nusing TSML: DateValNNer\nusing TSML: Statifier\n\ndtvalgator = DateValgator(Dict(:dateinterval => Dates.Hour(1)))\ndtvalnner = DateValNNer(Dict(:dateinterval => Dates.Hour(1)))\ndtvalizer = DateValizer(Dict(:dateinterval => Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing => true))\n\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            stfier\n         ]\n  )\n)\n\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)","category":"page"},{"location":"tutorial/statistics.html#Statifier-for-Non-Missing-Values-only-1","page":"Statistical Metrics","title":"Statifier for Non-Missing Values only","text":"","category":"section"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"If you are not intested with the statistics of the missing blocks, you can disable missing blocks stat summary by indicating :processmissing => false in the instance argument:","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>false))\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            stfier\n         ]\n  )\n)\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)","category":"page"},{"location":"tutorial/statistics.html#Statifier-After-Imputation-1","page":"Statistical Metrics","title":"Statifier After Imputation","text":"","category":"section"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let us check the statistics after the imputation by adding DateValNNer instance in the pipeline. We expect that if the imputation is successful, the stats for missing blocks will all be NaN because stats of empty set is an NaN.","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>true))\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            dtvalnner,\n            stfier\n         ]\n  )\n)\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"As we expected, the imputation is successful and there are no more missing values in the processed time series dataset.","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Let's try with the other imputation using DateValizer and validate that there are no more missing values based on the stats.","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"stfier = Statifier(Dict(:processmissing=>true))\nmypipeline = Pipeline(\n  Dict( :transformers => [\n            dtvalgator,\n            dtvalizer,\n            stfier\n         ]\n  )\n)\nfit!(mypipeline,X)\nresults = transform!(mypipeline,X)","category":"page"},{"location":"tutorial/statistics.html#","page":"Statistical Metrics","title":"Statistical Metrics","text":"Indeed, the imputation got rid of the missing values.","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/monotonic.html#Monotonic-Detection-1","page":"Monotonic Detection","title":"Monotonic Detection","text":"","category":"section"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"One important preprocessing step for time series data processing is the detection  of monotonic data and transform it to non-monotonic type by using the finite difference operator.","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Let's create an artificial monotonic data and apply our monotonic transformer to normalize it:","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"using Dates, DataFrames, Random\n\nRandom.seed!(123)\nmdates = DateTime(2017,12,31,1):Dates.Hour(1):DateTime(2017,12,31,10) |> collect\nmvals = rand(length(mdates)) |> cumsum\ndf =  DataFrame(Date=mdates ,Value = mvals)","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Now that we have a monotonic data, let's use the Monotonicer to normalize it:","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"using TSML, TSML.Utils, TSML.TSMLTypes\nusing TSML.TSMLTransformers\nusing TSML: Monotonicer\n\nmono = Monotonicer(Dict())\nfit!(mono,df)\nres=transform!(mono,df)","category":"page"},{"location":"tutorial/monotonic.html#Real-Data-Example-1","page":"Monotonic Detection","title":"Real Data Example","text":"","category":"section"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"We will now apply the entire pipeline  starting from reading csv data, aggregate, impute, and normalize if it's monotonic. We will consider three  different data types: a regular time series data, a   monotonic data, and a daily monotonic data. The difference between   monotonic and daily monotonic is that the values in daily monotonic resets to  zero or some baseline and cumulatively increases in a day until the  next day where it resets to zero or some baseline value. Monotonicer automatically detects these three different types and apply the corresponding normalization accordingly.","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"using TSML: DataReader\nusing TSML: DateValgator, DateValNNer, Statifier, Monotonicer\nregularfile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/regular.csv\")\nmonofile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/monotonic.csv\")\ndailymonofile = joinpath(dirname(pathof(TSML)),\"../data/typedetection/dailymonotonic.csv\")\n\nregularfilecsv = DataReader(Dict(:filename=>regularfile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\nmonofilecsv = DataReader(Dict(:filename=>monofile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\ndailymonofilecsv = DataReader(Dict(:filename=>dailymonofile,:dateformat=>\"dd/mm/yyyy HH:MM\"))\n\nvalgator = DateValgator(Dict(:dateinterval=>Dates.Hour(1)))\nvalnner = DateValNNer(Dict(:dateinterval=>Dates.Hour(1)))\nstfier = Statifier(Dict(:processmissing=>true))\nmono = Monotonicer(Dict())\nnothing #hide","category":"page"},{"location":"tutorial/monotonic.html#Regular-TS-Processing-1","page":"Monotonic Detection","title":"Regular TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Let's test by feeding the regular time series type to the pipeline. We expect that for this type, Monotonicer will not perform further processing:","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Pipeline with Monotonicer: regular time series","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"pipeline = Pipeline(Dict(\n    :transformers => [regularfilecsv,valgator,valnner,mono]\n   )\n)\nfit!(pipeline)\nregulardf=transform!(pipeline)\nfirst(regulardf,5)","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Pipeline without Monotonicer: regular time series","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"pipeline = Pipeline(Dict(\n    :transformers => [regularfilecsv,valgator,valnner]\n   )\n)\nfit!(pipeline)\nregulardf=transform!(pipeline)\nfirst(regulardf,5)","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Notice that the outputs are the same with or without the Monotonicer instance.","category":"page"},{"location":"tutorial/monotonic.html#Monotonic-TS-Processing-1","page":"Monotonic Detection","title":"Monotonic TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Let's now feed the same pipeline with a monotonic csv data.","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Pipeline with Monotonicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"pipeline = Pipeline(Dict(\n    :transformers => [monofilecsv,valgator,valnner,mono]\n   )\n)\nfit!(pipeline)\nmonodf=transform!(pipeline)\nfirst(monodf,10)","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Pipeline without Monotonicer: monotonic time series","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"pipeline = Pipeline(Dict(\n    :transformers => [monofilecsv,valgator,valnner]\n   )\n)\nfit!(pipeline)\nmonodf=transform!(pipeline)\nfirst(monodf,10)","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Notice that without the Monotonicer instance, the data becomes monotonic while with the Monotonicer instance in the pipeline, it becomes a regular time series data.","category":"page"},{"location":"tutorial/monotonic.html#Daily-Monotonic-TS-Processing-1","page":"Monotonic Detection","title":"Daily Monotonic TS Processing","text":"","category":"section"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Lastly, let's feed the daily monotonic data using similar pipeline and examine its output.","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Pipeline with Monotonicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"pipeline = Pipeline(Dict(\n    :transformers => [dailymonofilecsv,valgator,valnner,mono]\n   )\n)\nfit!(pipeline)\ndailymonodf=transform!(pipeline)\nfirst(dailymonodf,50)","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Pipeline without Monotonicer: daily monotonic time series","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"pipeline = Pipeline(Dict(\n    :transformers => [dailymonofilecsv,valgator,valnner]\n   )\n)\nfit!(pipeline)\ndailymonodf=transform!(pipeline)\nfirst(dailymonodf,50)","category":"page"},{"location":"tutorial/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Notice that the first 27 rows behave like a regular time series with no monotonic signature. Only after row 27 the data behaves in a monotonic fashion.  Notice further that the series reset to baseline value in row 38 at 1:00 am. This daily monotonic pattern can be seen when the data is plotted. In the pipeline with Monotonicer, the normalization replaces the baseline values to their immediate neighbor after applying the finite difference operation.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"tutorial/tsclassifier.html#TS-Data-Discovery-1","page":"TS Data Discovery","title":"TS Data Discovery","text":"","category":"section"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"We have enough building blocks to perform data discovery given a bunch  of time series data generated by sensors. Processing hundreds or thousands of time series data is becoming a common occurrence and typical challenge nowadays with the rapid adoption of IoT technology in buildings, manufacturing industries, etc.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"In this section, we will use those transformers discussed in the previous sections to normalize and extract the statistical features of TS. These extracted stat features will be used as input to a Machine learning model. We will train this model to learn the signatures of different TS types so that we can use it to classify unknown or unlabeled sensor data.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"In this tutorial, we will use TSClassifier which works in the following context:  Given a bunch of time-series with specific types. Get the statistical features of each, use these as inputs to a classifier with output as the TS type, train, and test. Another option is to use these stat features for clustering and check cluster quality. If accuracy is poor, add more stat features and repeat same process as outlined for training and testing. Assume that each time series during training is named based on their type which will be used as the target output. For example, temperature time series will be named as temperature?.csv where ? is any positive integer. Using this setup, the TSClassifier loops over each file in the training directory, get the stats and record these accumulated stat features into a dataframe and train the model to learn the input->output mapping during fit! operation. Apply the learned models in the transform! operation loading files in the testing directory.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"The entire process of training to learn the appropriate parameters and classification to identify unlabeled data exploits the idea of the pipeline workflow discussed in the previous sections.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let's illustrate the process by loading some sample data:","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"using Random\nusing TSML\n\nRandom.seed!(12345)\n\ntrdirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/training\")\ntstdirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/testing\")\nmodeldirname = joinpath(dirname(pathof(TSML)),\"../data/realdatatsclassification/model\")","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Here's the list of files for training:","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"show(readdir(trdirname) |> x->filter(y->match(r\".csv\",y) != nothing,x))","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"and here are the files in testing directory:","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"show(readdir(tstdirname) |> x->filter(y->match(r\".csv\",y) != nothing,x))","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"The files in testing directory doesn't need to be labeled but we use the labeling as a way to validate the effectiveness of the classifier. The labels will be used as the groundtruth during prediction/classification.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let us now setup an instance of the TSClassifier and pass the arguments containing the directory locations of files for training, testing, and modeling.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"using TSML: TSClassifier\nusing TSML: fit!, transform!\n\ntscl = TSClassifier(Dict(:trdirectory=>trdirname,\n          :tstdirectory=>tstdirname,\n          :modeldirectory=>modeldirname,\n          :feature_range => 6:20,\n          :num_trees=>20)\n       )\nnothing #hide","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Time to train our TSClassifier to learn the mapping between extracted stats features with the  TS type.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"fit!(tscl)","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"We can examine the extracted features saved by the model that is used for its training.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"using CSV, DataFrames\n\nmdirname = tscl.args[:modeldirectory]\nmodelfname=tscl.args[:juliarfmodelname]\n\ntrstatfname = joinpath(mdirname,modelfname*\".csv\")\nCSV.read(trstatfname) |> DataFrame","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Let's check the accuracy of prediction with the test data using the transform! function.","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"dfresults = transform!(tscl)","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"The table above shows the prediction corresponding to each filename which is the groundtruth. We can compute the accuracy by extracting from the filename the TS type and compare it with the corresponding prediction. Below computes the prediction accuracy:","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"prediction = dfresults[:predtype]\nfnames = dfresults[:fname]\nmyregex = r\"(?<dtype>[A-Z _ - a-z]+)(?<number>\\d*).(?<ext>\\w+)\"\ngroundtruth=map(fnames) do fname\n  mymatch=match(myregex,fname)\n  mymatch[:dtype]\nend\nsum(groundtruth .== prediction) / length(groundtruth) * 100","category":"page"},{"location":"tutorial/tsclassifier.html#","page":"TS Data Discovery","title":"TS Data Discovery","text":"Of course we need more data to split between training and testing to improve accuracy and get a more stable measurement of performance.","category":"page"},{"location":"man/dateproc.html#","page":"Date Processing","title":"Date Processing","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/dateproc.html#Preprocessing-1","page":"Date Processing","title":"Preprocessing","text":"","category":"section"},{"location":"man/valueproc.html#","page":"Value Processing","title":"Value Processing","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/valueproc.html#Value-Preprocessing-1","page":"Value Processing","title":"Value Preprocessing","text":"","category":"section"},{"location":"man/aggregation.html#","page":"Aggregation","title":"Aggregation","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/aggregation.html#Aggregation-1","page":"Aggregation","title":"Aggregation","text":"","category":"section"},{"location":"man/imputation.html#","page":"Imputation","title":"Imputation","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/imputation.html#Imputation-1","page":"Imputation","title":"Imputation","text":"","category":"section"},{"location":"man/monotonic.html#","page":"Monotonic Detection","title":"Monotonic Detection","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/monotonic.html#Monotonic-Detection-1","page":"Monotonic Detection","title":"Monotonic Detection","text":"","category":"section"},{"location":"man/tsclassification.html#","page":"TS Classification","title":"TS Classification","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/tsclassification.html#TS-Classification-1","page":"TS Classification","title":"TS Classification","text":"","category":"section"},{"location":"man/cli.html#","page":"CLI Wrappers","title":"CLI Wrappers","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"man/cli.html#CLI-Wrappers-1","page":"CLI Wrappers","title":"CLI Wrappers","text":"","category":"section"},{"location":"lib/decisiontree.html#","page":"Decision Tree","title":"Decision Tree","text":"Author = \"Paulito Palmes\"","category":"page"},{"location":"lib/decisiontree.html#lib_decisiontree-1","page":"Decision Tree","title":"DecisionTreeLearners","text":"","category":"section"},{"location":"lib/decisiontree.html#","page":"Decision Tree","title":"Decision Tree","text":"Creates an API wrapper for DecisionTrees for pipeline workflow.","category":"page"},{"location":"lib/decisiontree.html#","page":"Decision Tree","title":"Decision Tree","text":"Modules = [DecisionTreeLearners]","category":"page"},{"location":"lib/decisiontree.html#","page":"Decision Tree","title":"Decision Tree","text":"Modules = [DecisionTreeLearners]","category":"page"}]
}
